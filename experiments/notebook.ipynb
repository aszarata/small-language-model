{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2be6153",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aszarata/informatyka/9s/lingwistyka/lab02/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.utils.model_utils import load_model\n",
    "from tokenizers import Tokenizer\n",
    "from src.data.text_dataset import TextDataset\n",
    "from src.data.hf_dataset_processor import HFDatasetProcessor\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d82a19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\"../models/model-bpe/tokenizer.json\")\n",
    "model = load_model(\"../models/model-bpe/checkpoint_6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b37ca6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109158736"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([p.numel() for p in model.parameters()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cbac868",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TextDataset(\n",
    "    data_dir=\"../data/base/test\",\n",
    "    tokenizer=tokenizer,\n",
    "    seq_len=128\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=dataset, \n",
    "    batch_size=1,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b5be3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset:   3%|▎         | 46000/1562327 [00:15<08:25, 3000.26 examples/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m dataset2 = \u001b[43mHFDatasetProcessor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../data/plwiki\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m.get_data()\n\u001b[32m      6\u001b[39m dataloader = DataLoader(\n\u001b[32m      7\u001b[39m     dataset=dataset2, \n\u001b[32m      8\u001b[39m     batch_size=\u001b[32m4\u001b[39m,\n\u001b[32m      9\u001b[39m     shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     10\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/informatyka/9s/lingwistyka/lab02/experiments/../src/data/hf_dataset_processor.py:18\u001b[39m, in \u001b[36mHFDatasetProcessor.__init__\u001b[39m\u001b[34m(self, data_path, tokenizer, max_seq_len)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mself\u001b[39m.tokenizer.enable_padding(\n\u001b[32m     13\u001b[39m     direction=\u001b[33m'\u001b[39m\u001b[33mright\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     14\u001b[39m     pad_id=tokenizer.token_to_id(\u001b[33m\"\u001b[39m\u001b[33m<pad>\u001b[39m\u001b[33m\"\u001b[39m), \n\u001b[32m     15\u001b[39m     length=max_seq_len + \u001b[32m1\u001b[39m\n\u001b[32m     16\u001b[39m )\n\u001b[32m     17\u001b[39m \u001b[38;5;28mself\u001b[39m.dataset = load_from_disk(data_path)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28mself\u001b[39m.tokenized_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/informatyka/9s/lingwistyka/lab02/experiments/../src/data/hf_dataset_processor.py:37\u001b[39m, in \u001b[36mHFDatasetProcessor._process_dataset\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_process_dataset\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     tokenized_dataset = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tokenize_and_prepare\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m        \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumn_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTokenizing dataset\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m     tokenized_dataset.set_format(\u001b[38;5;28mtype\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m\"\u001b[39m, columns=[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_dataset\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/informatyka/9s/lingwistyka/lab02/.venv/lib/python3.13/site-packages/datasets/arrow_dataset.py:562\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    555\u001b[39m self_format = {\n\u001b[32m    556\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    557\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    558\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    559\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    560\u001b[39m }\n\u001b[32m    561\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m562\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    563\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/informatyka/9s/lingwistyka/lab02/.venv/lib/python3.13/site-packages/datasets/arrow_dataset.py:3341\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[39m\n\u001b[32m   3339\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3340\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m unprocessed_kwargs \u001b[38;5;129;01min\u001b[39;00m unprocessed_kwargs_per_job:\n\u001b[32m-> \u001b[39m\u001b[32m3341\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43munprocessed_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3342\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcheck_if_shard_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3344\u001b[39m \u001b[38;5;66;03m# Avoids PermissionError on Windows (the error: https://github.com/huggingface/datasets/actions/runs/4026734820/jobs/6921621805)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/informatyka/9s/lingwistyka/lab02/.venv/lib/python3.13/site-packages/datasets/arrow_dataset.py:3697\u001b[39m, in \u001b[36mDataset._map_single\u001b[39m\u001b[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[39m\n\u001b[32m   3695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3696\u001b[39m     _time = time.time()\n\u001b[32m-> \u001b[39m\u001b[32m3697\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miter_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_iterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3698\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_examples_in_batch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3699\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mupdate_data\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/informatyka/9s/lingwistyka/lab02/.venv/lib/python3.13/site-packages/datasets/arrow_dataset.py:3647\u001b[39m, in \u001b[36mDataset._map_single.<locals>.iter_outputs\u001b[39m\u001b[34m(shard_iterable)\u001b[39m\n\u001b[32m   3645\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3646\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[32m-> \u001b[39m\u001b[32m3647\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m i, \u001b[43mapply_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/informatyka/9s/lingwistyka/lab02/.venv/lib/python3.13/site-packages/datasets/arrow_dataset.py:3570\u001b[39m, in \u001b[36mDataset._map_single.<locals>.apply_function\u001b[39m\u001b[34m(pa_inputs, indices, offset)\u001b[39m\n\u001b[32m   3568\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[32m   3569\u001b[39m inputs, fn_args, additional_args, fn_kwargs = prepare_inputs(pa_inputs, indices, offset=offset)\n\u001b[32m-> \u001b[39m\u001b[32m3570\u001b[39m processed_inputs = \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3571\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/informatyka/9s/lingwistyka/lab02/experiments/../src/data/hf_dataset_processor.py:21\u001b[39m, in \u001b[36mHFDatasetProcessor._tokenize_and_prepare\u001b[39m\u001b[34m(self, examples)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_tokenize_and_prepare\u001b[39m(\u001b[38;5;28mself\u001b[39m, examples: Dict[\u001b[38;5;28mstr\u001b[39m, List[Any]]) -> Dict[\u001b[38;5;28mstr\u001b[39m, List[List[\u001b[38;5;28mint\u001b[39m]]]:\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     tokenized_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m     input_ids = [encoding.ids \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m tokenized_output]\n\u001b[32m     28\u001b[39m     labels = [ids[\u001b[32m1\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m ids \u001b[38;5;129;01min\u001b[39;00m input_ids]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "dataset2 = HFDatasetProcessor(\n",
    "    \"../data/plwiki\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=128\n",
    ").get_data()\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset2, \n",
    "    batch_size=4,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c74ae9ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m x, y = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(test_loader))\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m x2, y2 = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[43mdataloader\u001b[49m))\n",
      "\u001b[31mNameError\u001b[39m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(test_loader))\n",
    "x2, y2 = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42e2c3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 128]) torch.Size([4, 128])\n"
     ]
    }
   ],
   "source": [
    "print(x2.shape, y2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b84534b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([13325,  8951,  1267,   678,    17,  2094,    85, 17726,    12, 18095,\n",
      "        10623,  1241, 19251,   446, 45940, 25497,    15, 27159,   155, 39674,\n",
      "           16, 44778,  2142,  2618,  1958,    15,   181, 20571,  3285, 35473,\n",
      "          498, 45940, 18599,    15,  4673,   859,    67,  1572,  7381, 26283,\n",
      "          310,   453,   504,   222,    17,   346,   454,  1714,  8211,   196,\n",
      "         5783,  2659,    17,  8470, 30764,  5117,    17,   127,   127,    42,\n",
      "           69, 13257,    65,   127,   127,  8043,   206,  3068, 14185,  1241,\n",
      "          129,   127, 34862,   287,   446, 13325,  8951,  3872,   155, 39674,\n",
      "           16, 44778,  2142,  2618, 45940, 25497,    17,   462,  6619,  2198,\n",
      "          226,   159,  7543, 21343,  1212,   824, 23710,    17,   462, 13006,\n",
      "         1630,    90,  7083,  9093,  1694,  5146,   159,  2448,   163,   492,\n",
      "          170,   442, 23485,  2106,    15,   470,   181,  5888,   159,  2408,\n",
      "         2171,   330,  1715,   170,   181, 12018,   159,   310]) tensor([ 8951,  1267,   678,    17,  2094,    85, 17726,    12, 18095, 10623,\n",
      "         1241, 19251,   446, 45940, 25497,    15, 27159,   155, 39674,    16,\n",
      "        44778,  2142,  2618,  1958,    15,   181, 20571,  3285, 35473,   498,\n",
      "        45940, 18599,    15,  4673,   859,    67,  1572,  7381, 26283,   310,\n",
      "          453,   504,   222,    17,   346,   454,  1714,  8211,   196,  5783,\n",
      "         2659,    17,  8470, 30764,  5117,    17,   127,   127,    42,    69,\n",
      "        13257,    65,   127,   127,  8043,   206,  3068, 14185,  1241,   129,\n",
      "          127, 34862,   287,   446, 13325,  8951,  3872,   155, 39674,    16,\n",
      "        44778,  2142,  2618, 45940, 25497,    17,   462,  6619,  2198,   226,\n",
      "          159,  7543, 21343,  1212,   824, 23710,    17,   462, 13006,  1630,\n",
      "           90,  7083,  9093,  1694,  5146,   159,  2448,   163,   492,   170,\n",
      "          442, 23485,  2106,    15,   470,   181,  5888,   159,  2408,  2171,\n",
      "          330,  1715,   170,   181, 12018,   159,   310,   453])\n"
     ]
    }
   ],
   "source": [
    "print(x2[3], y2[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3695d874",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(x)\n",
    "\n",
    "log_probs = F.log_softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e0470d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " którzy na komorne wprawdzie nie mają, ale urządzać u siebie hałasy i pijatyki co tydzień to mogą, ujmuje się za Ignacym, starym leniem i niedołęgą, przez którego marnieje ogród, nie da słowa powiedzieć żadnej służącej. Co to wszystko jest? Poczucie sprawiedliwości, równość, uświadomienie społeczne? Pani Cecylii wiadome były te rzeczy nie od dzisiaj. Sprawiedliwość, dobrze. Ale niechże idzie ze swoimi pretensjami tam, gdzie naprawdę jest jakiś wyzysk i krzywda, nie do niej, którą właśnie wszyscy wyzyskują, której ta kamienica nic nie przynosi prócz kłopotów. „Ja ci wcale nie powiadam — mówiła do Elżbiety\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(x[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e32bbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " na komorne wprawdzie nie mają, ale urządzać u siebie hałasy i pijatyki co tydzień to mogą, ujmuje się za Ignacym, starym leniem i niedołęgą, przez którego marnieje ogród, nie da słowa powiedzieć żadnej służącej. Co to wszystko jest? Poczucie sprawiedliwości, równość, uświadomienie społeczne? Pani Cecylii wiadome były te rzeczy nie od dzisiaj. Sprawiedliwość, dobrze. Ale niechże idzie ze swoimi pretensjami tam, gdzie naprawdę jest jakiś wyzysk i krzywda, nie do niej, którą właśnie wszyscy wyzyskują, której ta kamienica nic nie przynosi prócz kłopotów. „Ja ci wcale nie powiadam — mówiła do Elżbiety —\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(y[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b81b321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 50000])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(log_probs.shape)\n",
    "predicted_tokens = log_probs.argmax(dim=-1)\n",
    "\n",
    "decoded_text = tokenizer.decode(predicted_tokens[0].tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "039dffa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(nan)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = (y != tokenizer.token_to_id(\"<pad>\"))\n",
    "target_log_probs = log_probs.gather(\n",
    "    dim=-1,\n",
    "    index=y.unsqueeze(-1)\n",
    "\n",
    ").squeeze(-1)\n",
    "masked_log_probs = target_log_probs * mask\n",
    "\n",
    "negative_log_likelihood = -masked_log_probs.sum()\n",
    "meana = negative_log_likelihood.item() / mask.sum()\n",
    "torch.exp(meana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "705f72cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan]], grad_fn=<ExpBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(target_log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b496d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(nan, grad_fn=<MaxBackward1>)\n",
      "tensor(0)\n",
      "tensor(nan, grad_fn=<SelectBackward0>)\n",
      "tensor(181)\n",
      "<class 'int'>\n",
      " na\n"
     ]
    }
   ],
   "source": [
    "probs = F.softmax(logits, dim=-1)\n",
    "print(torch.max(probs[0][0]))\n",
    "print(torch.argmax(probs[0][0]))\n",
    "print(probs[0][0][2810])\n",
    "print(y[0][0])\n",
    "print(type(y[0][0].item()))\n",
    "print(tokenizer.decode([y[0][0].item()]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
